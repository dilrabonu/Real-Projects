{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTvOy8Z24EuVCjPmP3lcjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dilrabonu/Real-Projects/blob/main/Streaming_Parctical2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SnA1RNBkqOIm"
      },
      "outputs": [],
      "source": [
        "# âœ… Correct Spark and Java installation\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.3.2-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Set environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.2-bin-hadoop3\""
      ],
      "metadata": {
        "id": "-XM04y1fqRUF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# âœ… Start SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReceiptETL\").getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "0hccUj34q_jY",
        "outputId": "e4286715-e54f-4743-853c-d04038cd023d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7bc6b2b38cd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://38b58aa4ad9e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ReceiptETL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!file /content/Spark_Streaming--Dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrHOJV0trATg",
        "outputId": "3aa75b82-8e4a-4af4-aa5c-78d1936e0a3c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Spark_Streaming--Dataset.zip: Zip archive data, at least v1.0 to extract, compression method=store\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/Spark_Streaming--Dataset.zip\"\n",
        "extract_path = \"/content/\"\n",
        "\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"âœ… Extraction Successful!\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"âŒ The file is not a valid ZIP archive.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHvxLhcbrHl0",
        "outputId": "1fca7016-1fe5-419c-d5b3-2c1c67642439"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction Successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Define paths\n",
        "files_to_extract = [\"/content/weather.zip\"]\n",
        "extract_path = \"/content/\"\n",
        "\n",
        "# Extract each file\n",
        "for file in files_to_extract:\n",
        "    try:\n",
        "        with zipfile.ZipFile(file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_path)\n",
        "        print(f\"âœ… Successfully extracted {file}!\")\n",
        "    except zipfile.BadZipFile:\n",
        "        print(f\"âŒ Failed to extract {file}: Not a valid ZIP archive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NzeuZ8LrLBk",
        "outputId": "99436d73-2c17-4a5f-8d2a-9ae0ffc169dc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully extracted /content/weather.zip!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Define the zip file and extraction path\n",
        "zip_path = \"/content/receipt_restaurants.zip\"\n",
        "extract_path = \"/content/receipt_restaurants\"\n",
        "\n",
        "# Extract\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "    print(\"âœ… Successfully extracted receipt_restaurants.zip!\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"âŒ The file is not a valid ZIP archive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xVrluU3rNHt",
        "outputId": "e35580a4-7654-4bb8-dd1e-fe3644c99512"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Successfully extracted receipt_restaurants.zip!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Load CSV Data\n",
        "\n",
        "In this step, we load the restaurant receipt data and weather data from CSV files into Spark DataFrames. We also inspect the schema and the first few rows to understand the structure of the data."
      ],
      "metadata": {
        "id": "huxoNFb8sT6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Load receipt_restaurants data\n",
        "receipt_df = spark.read.option(\"header\", True).csv(\"/content/receipt_restaurants/part-*.csv\")\n",
        "receipt_df.printSchema()\n",
        "receipt_df.show(5, truncate=False)\n",
        "\n",
        "# STEP 2: Load weather data\n",
        "weather_df = spark.read.option(\"header\", True).csv(\"/content/weather/part-*.csv\")\n",
        "weather_df.printSchema()\n",
        "weather_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhwM0UHHra2-",
        "outputId": "a06490cf-a631-4484-cb64-53423d46ea0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- franchise_id: string (nullable = true)\n",
            " |-- franchise_name: string (nullable = true)\n",
            " |-- restaurant_franchise_id: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- lng: string (nullable = true)\n",
            " |-- receipt_id: string (nullable = true)\n",
            " |-- total_cost: string (nullable = true)\n",
            " |-- discount: string (nullable = true)\n",
            " |-- date_time: string (nullable = true)\n",
            "\n",
            "+------------+------------+-----------------+-----------------------+-------+---------+------+--------+------------------------------------+----------+--------+------------------------+\n",
            "|id          |franchise_id|franchise_name   |restaurant_franchise_id|country|city     |lat   |lng     |receipt_id                          |total_cost|discount|date_time               |\n",
            "+------------+------------+-----------------+-----------------------+-------+---------+------+--------+------------------------------------+----------+--------+------------------------+\n",
            "|188978561075|52          |The Red Door     |5034                   |AT     |Vienna   |48.215|16.376  |56df62bf-f7e7-47ff-8800-475bf46262cf|17.40     |0.15    |2022-09-05T17:32:50.000Z|\n",
            "|214748364857|58          |A Tasty Bite     |23242                  |FR     |Paris    |48.866|2.332   |f3ed7e84-f3c7-46e7-b855-f6def62911bb|17.12     |0.0     |2021-10-01T18:53:23.000Z|\n",
            "|266287972391|40          |Crimson Cafe     |70700                  |IT     |Milan    |45.473|9.191   |4cbfe14a-77ab-489e-aeb8-192931ad493a|13.90     |0.0     |2022-09-07T07:02:40.000Z|\n",
            "|17179869216 |33          |The Blue Elephant|66775                  |US     |Hill City|43.959|-103.737|397c3559-92fc-4f4d-9bf7-3ad47cb51620|20.78     |0.15    |2021-10-11T19:19:32.000Z|\n",
            "|25769803801 |26          |The Silver Spoon |95123                  |ES     |Barcelona|41.401|2.209   |10ef1be3-83d3-47db-a7d0-de7d71924f91|10.84     |0.1     |2021-10-04T09:57:45.000Z|\n",
            "+------------+------------+-----------------+-----------------------+-------+---------+------+--------+------------------------------------+----------+--------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- lng: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- avg_tmpr_c: string (nullable = true)\n",
            " |-- wthr_date: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n",
            "+------+------+----------+----------+------+-------+\n",
            "|lng   |lat   |avg_tmpr_c|wthr_date |city  |country|\n",
            "+------+------+----------+----------+------+-------+\n",
            "|2.326 |48.847|7.01      |2021-10-22|Paris |FR     |\n",
            "|2.352 |48.864|18.75     |2022-09-28|Paris |FR     |\n",
            "|2.328 |48.871|8.08      |2021-10-12|Paris |FR     |\n",
            "|-0.152|51.506|11.93     |2021-10-15|London|GB     |\n",
            "|2.307 |48.855|11.12     |2021-10-02|Paris |FR     |\n",
            "+------+------+----------+----------+------+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Clean and Prepare Data\n",
        "\n",
        "In this step, we round the latitude and longitude values to 2 decimal places for both datasets to ensure consistent join keys.\n",
        "\n",
        "We also convert the `date_time` column in the receipt data to a proper `visit_date`, and do the same for the weather data using the `wthr_date` column.\n",
        "\n",
        "These transformations are necessary to prepare for the join operation in the next step.\n"
      ],
      "metadata": {
        "id": "4WqkGBGwsqlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, round, to_date\n",
        "\n",
        "# Round lat/lng and convert datetime to date\n",
        "receipt_df = receipt_df.withColumn(\"lat_round\", round(col(\"lat\").cast(\"double\"), 2)) \\\n",
        "                       .withColumn(\"lng_round\", round(col(\"lng\").cast(\"double\"), 2)) \\\n",
        "                       .withColumn(\"visit_date\", to_date(\"date_time\"))\n",
        "\n",
        "weather_df = weather_df.withColumn(\"lat_round\", round(col(\"lat\").cast(\"double\"), 2)) \\\n",
        "                       .withColumn(\"lng_round\", round(col(\"lng\").cast(\"double\"), 2)) \\\n",
        "                       .withColumn(\"wthr_date\", to_date(\"wthr_date\"))\n"
      ],
      "metadata": {
        "id": "IuaBh5Ter4nR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Join Receipt and Weather Data\n",
        "\n",
        "To enrich our receipt records with weather information, we perform a left join between the `receipt_df` and `weather_df` on the following keys:\n",
        "- Rounded latitude (`lat_round`)\n",
        "- Rounded longitude (`lng_round`)\n",
        "- Converted visit date (`visit_date` == `wthr_date`)\n",
        "\n",
        "We use aliasing to reference each DataFrame cleanly during the join.\n",
        "\n",
        "This step enables us to later analyze how temperature affects customer behavior.\n"
      ],
      "metadata": {
        "id": "5qdz0PAUs4rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create aliases\n",
        "receipt_df_alias = receipt_df.alias(\"r\")\n",
        "weather_df_alias = weather_df.alias(\"w\")\n",
        "\n",
        "# Join dataframes\n",
        "enriched_df = receipt_df_alias.join(\n",
        "    weather_df_alias,\n",
        "    (col(\"r.lat_round\") == col(\"w.lat_round\")) &\n",
        "    (col(\"r.lng_round\") == col(\"w.lng_round\")) &\n",
        "    (col(\"r.visit_date\") == col(\"w.wthr_date\")),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Preview joined data\n",
        "enriched_df.select(\n",
        "    col(\"r.receipt_id\"),\n",
        "    col(\"r.visit_date\"),\n",
        "    col(\"w.avg_tmpr_c\"),\n",
        "    col(\"w.city\").alias(\"weather_city\"),\n",
        "    col(\"r.city\").alias(\"receipt_city\")\n",
        ").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLmz_LszsYss",
        "outputId": "88abe8d7-2e98-4ed2-b152-676a0a6d26ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+----------+----------+------------+------------+\n",
            "|receipt_id                          |visit_date|avg_tmpr_c|weather_city|receipt_city|\n",
            "+------------------------------------+----------+----------+------------+------------+\n",
            "|56df62bf-f7e7-47ff-8800-475bf46262cf|2022-09-05|null      |null        |Vienna      |\n",
            "|f3ed7e84-f3c7-46e7-b855-f6def62911bb|2021-10-01|null      |null        |Paris       |\n",
            "|4cbfe14a-77ab-489e-aeb8-192931ad493a|2022-09-07|null      |null        |Milan       |\n",
            "|397c3559-92fc-4f4d-9bf7-3ad47cb51620|2021-10-11|null      |null        |Hill City   |\n",
            "|10ef1be3-83d3-47db-a7d0-de7d71924f91|2021-10-04|null      |null        |Barcelona   |\n",
            "+------------------------------------+----------+----------+------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Filter Enriched Data\n",
        "\n",
        "To ensure data quality, we remove any records with missing or invalid temperature readings.\n",
        "We keep only the rows where:\n",
        "- `avg_tmpr_c` is **not null**\n",
        "- `avg_tmpr_c` is **greater than 0**\n",
        "\n",
        "This results in a clean dataset suitable for further analysis or reporting.\n"
      ],
      "metadata": {
        "id": "YwdRe77RtS8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out records with invalid temperature values\n",
        "filtered_df = enriched_df.filter(col(\"avg_tmpr_c\") > 0)\n",
        "\n",
        "# Preview cleaned data\n",
        "filtered_df.select(\"receipt_id\", \"visit_date\", \"avg_tmpr_c\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_oYIqL3suaB",
        "outputId": "258a3cf1-7ab1-4f9d-a4d2-5e73fdaf7274"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------------------------+----------+----------+\n",
            "|receipt_id                          |visit_date|avg_tmpr_c|\n",
            "+------------------------------------+----------+----------+\n",
            "|885ffe85-3320-49a5-bc91-8f8ed227af5a|2021-10-11|10.81     |\n",
            "|7e4432fc-0649-4eab-883c-e3e975501413|2022-09-01|30.01     |\n",
            "|d7da2e39-1c0c-4f11-8826-d174abc97e40|2022-08-17|15.91     |\n",
            "|d7da2e39-1c0c-4f11-8826-d174abc97e40|2022-08-17|16.44     |\n",
            "|d7da2e39-1c0c-4f11-8826-d174abc97e40|2022-08-17|16.03     |\n",
            "+------------------------------------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Derive Original Total Cost\n",
        "\n",
        "We create a new column called `original_total_cost` to represent the cost before applying discounts.\n",
        "\n",
        "Formula used:\n"
      ],
      "metadata": {
        "id": "SmqL7rg0tfs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Add original_total_cost column (total_cost + discount)\n",
        "filtered_df = filtered_df.withColumn(\n",
        "    \"original_total_cost\",\n",
        "    col(\"r.total_cost\").cast(\"double\") + col(\"r.discount\").cast(\"double\")\n",
        ")\n",
        "\n",
        "# Preview\n",
        "filtered_df.select(\"receipt_id\", \"total_cost\", \"discount\", \"original_total_cost\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9Ujz93mtDNv",
        "outputId": "8285f244-02bd-4cee-a889-4399f59c474d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+--------+-------------------+\n",
            "|          receipt_id|total_cost|discount|original_total_cost|\n",
            "+--------------------+----------+--------+-------------------+\n",
            "|885ffe85-3320-49a...|     15.65|     0.0|              15.65|\n",
            "|7e4432fc-0649-4ea...|     27.93|     0.0|              27.93|\n",
            "|d7da2e39-1c0c-4f1...|     21.84|     0.0|              21.84|\n",
            "|d7da2e39-1c0c-4f1...|     21.84|     0.0|              21.84|\n",
            "|d7da2e39-1c0c-4f1...|     21.84|     0.0|              21.84|\n",
            "+--------------------+----------+--------+-------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Derive Item Count and Order Type\n",
        "\n",
        "Since the original dataset doesn't include an `item_count`, we simulate it using a random integer from 0 to 10.\n",
        "\n",
        "Then, we classify each order into a category using conditional logic:\n",
        "- `Tiny order` for 1 or fewer items\n",
        "- `Small order` for up to 3 items\n",
        "- `Medium order` for up to 10 items\n",
        "- `Large order` for more than 10 items\n",
        "- `Erroneous data` if the item count is null or 0\n"
      ],
      "metadata": {
        "id": "wE0555oOtt5Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, when\n",
        "\n",
        "# Step 6.1: Create item_count from random values\n",
        "filtered_df = filtered_df.withColumn(\"item_count\", (rand() * 10).cast(\"int\"))\n",
        "\n",
        "# Step 6.2: Classify order_type based on item_count\n",
        "filtered_df = filtered_df.withColumn(\n",
        "    \"order_type\",\n",
        "    when(col(\"item_count\").isNull() | (col(\"item_count\") <= 0), \"Erroneous data\")\n",
        "    .when(col(\"item_count\") <= 1, \"Tiny order\")\n",
        "    .when(col(\"item_count\") <= 3, \"Small order\")\n",
        "    .when(col(\"item_count\") <= 10, \"Medium order\")\n",
        "    .otherwise(\"Large order\")\n",
        ")\n",
        "\n",
        "# Preview\n",
        "filtered_df.select(\"receipt_id\", \"item_count\", \"order_type\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt50lc4PtYEg",
        "outputId": "984ecfe7-0e0d-4551-f6d7-8d95e4bb2060"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+--------------+\n",
            "|          receipt_id|item_count|    order_type|\n",
            "+--------------------+----------+--------------+\n",
            "|885ffe85-3320-49a...|         8|  Medium order|\n",
            "|7e4432fc-0649-4ea...|         0|Erroneous data|\n",
            "|d7da2e39-1c0c-4f1...|         1|    Tiny order|\n",
            "|d7da2e39-1c0c-4f1...|         3|   Small order|\n",
            "|d7da2e39-1c0c-4f1...|         7|  Medium order|\n",
            "+--------------------+----------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Aggregate Order Type Counts by Franchise\n",
        "\n",
        "We group data by `restaurant_franchise_id` and pivot on `order_type` to get counts for each category:\n",
        "- `Tiny order`\n",
        "- `Small order`\n",
        "- `Medium order`\n",
        "- `Large order`\n",
        "- `Erroneous data`\n",
        "\n",
        "This will help in identifying which type of order is most frequent per franchise.\n"
      ],
      "metadata": {
        "id": "sF8gP1VEuCfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Group by restaurant_franchise_id and count order types\n",
        "order_counts = filtered_df.groupBy(\"r.restaurant_franchise_id\").pivot(\"order_type\").count()\n",
        "\n",
        "# Preview\n",
        "order_counts.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBXylZ-ztmWn",
        "outputId": "bc512ae7-59c0-4df6-9c64-9812fd41449e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+--------------+------------+-----------+----------+\n",
            "|restaurant_franchise_id|Erroneous data|Medium order|Small order|Tiny order|\n",
            "+-----------------------+--------------+------------+-----------+----------+\n",
            "|22875                  |1207          |6969        |2389       |1190      |\n",
            "|76199                  |1254          |7575        |2541       |1312      |\n",
            "|23242                  |2771          |16453       |5526       |2762      |\n",
            "|80392                  |1564          |9825        |3324       |1594      |\n",
            "|48721                  |980           |5497        |1833       |930       |\n",
            "+-----------------------+--------------+------------+-----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Identify the Most Popular Order Type per Franchise\n",
        "\n",
        "Now that we have counts for each order type, we determine which one is the most popular per `restaurant_franchise_id`.\n",
        "\n",
        "This is done using chained `when()` conditions to compare all order type columns and extract the highest one. This step helps answer business questions like:\n",
        "- What kind of orders are most common in each franchise?\n",
        "- Where do we see frequent erroneous or unusual data?\n"
      ],
      "metadata": {
        "id": "BBUMtH8suQjV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import when\n",
        "\n",
        "# Fill nulls to avoid comparison issues\n",
        "final_state_df = order_counts.fillna(0)\n",
        "\n",
        "# Add most_popular_order_type column\n",
        "final_state_df = final_state_df.withColumn(\n",
        "    \"most_popular_order_type\",\n",
        "    when(\n",
        "        (col(\"Medium order\") >= col(\"Small order\")) &\n",
        "        (col(\"Medium order\") >= col(\"Tiny order\")) &\n",
        "        (col(\"Medium order\") >= col(\"Erroneous data\")), \"Medium order\"\n",
        "    ).when(\n",
        "        (col(\"Small order\") >= col(\"Tiny order\")) &\n",
        "        (col(\"Small order\") >= col(\"Erroneous data\")), \"Small order\"\n",
        "    ).when(\n",
        "        (col(\"Tiny order\") >= col(\"Erroneous data\")), \"Tiny order\"\n",
        "    ).otherwise(\"Erroneous data\")\n",
        ")\n",
        "\n",
        "# Preview\n",
        "final_state_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWIlWuadt0bU",
        "outputId": "d3bb2525-47fa-4b32-85e6-96e484687e09"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+--------------+------------+-----------+----------+-----------------------+\n",
            "|restaurant_franchise_id|Erroneous data|Medium order|Small order|Tiny order|most_popular_order_type|\n",
            "+-----------------------+--------------+------------+-----------+----------+-----------------------+\n",
            "|22875                  |1207          |6969        |2389       |1190      |Medium order           |\n",
            "|76199                  |1254          |7575        |2541       |1312      |Medium order           |\n",
            "|23242                  |2771          |16453       |5526       |2762      |Medium order           |\n",
            "|80392                  |1564          |9825        |3324       |1594      |Medium order           |\n",
            "|48721                  |980           |5497        |1833       |930       |Medium order           |\n",
            "+-----------------------+--------------+------------+-----------+----------+-----------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 9: Save the Final Output\n",
        "\n",
        "After deriving the most popular order type for each franchise, we save the resulting DataFrame into a single `.csv` file using `.coalesce(1)` to ensure one output file. This makes the output clean and easy to use in any IDE or analysis tool.\n"
      ],
      "metadata": {
        "id": "4jnX1CwrunUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final result\n",
        "final_state_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", True).csv(\"/content/output/final_state\")\n"
      ],
      "metadata": {
        "id": "eV62J9FAuHF7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… Step 1: Read 2021 Receipts in Streaming Mode\n",
        "\n",
        "We begin processing the receipts for 2021 using Spark Structured Streaming. Since streaming requires a pre-defined schema, we extract it by reading one static batch file from the 2021 dataset.\n",
        "\n",
        "We use this schema to set up the `readStream()` function and load the data in real time for further transformation and enrichment.\n"
      ],
      "metadata": {
        "id": "Ck9-9_xvwMja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1.1: Read 2021 data using Spark Streaming\n",
        "from pyspark.sql.functions import col, round, to_date\n",
        "\n",
        "# First define schema using batch read (already done)\n",
        "receipt_2021_df = spark.read.option(\"header\", True).csv(\"/content/receipt_restaurants/part-00000*.csv\")\n",
        "\n",
        "# Then stream with schema\n",
        "receipt_2021_stream_df = spark.readStream \\\n",
        "    .option(\"header\", True) \\\n",
        "    .schema(receipt_2021_df.schema) \\\n",
        "    .csv(\"/content/receipt_restaurants/part-00000*.csv\")\n"
      ],
      "metadata": {
        "id": "b0xs99kOuZsQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### âœ… Step 2: Clean and Enrich 2021 Streaming Data\n",
        "\n",
        "We apply the same preprocessing and enrichment steps used in batch:\n",
        "\n",
        "- Round latitude and longitude to 2 decimal places.\n",
        "- Convert `date_time` to `visit_date`.\n",
        "- Join streaming receipts with static weather data.\n",
        "- Filter records with `avg_tmpr_c > 0`.\n",
        "- Compute `original_total_cost` = `total_cost + discount`.\n",
        "\n",
        "These steps ensure that streaming and batch data are handled consistently and satisfy the logic defined in the task.\n"
      ],
      "metadata": {
        "id": "GSbaE6sZwePy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr, when\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "# Step 2.1: Clean streaming data (round, to_date)\n",
        "receipt_2021_stream_df = receipt_2021_stream_df \\\n",
        "    .withColumn(\"lat_round\", round(col(\"lat\").cast(\"double\"), 2)) \\\n",
        "    .withColumn(\"lng_round\", round(col(\"lng\").cast(\"double\"), 2)) \\\n",
        "    .withColumn(\"visit_date\", to_date(\"date_time\"))\n",
        "\n",
        "# Step 2.2: Clean weather data again (ensure correct columns)\n",
        "weather_df_cleaned = weather_df \\\n",
        "    .withColumn(\"lat_round\", round(col(\"lat\").cast(\"double\"), 2)) \\\n",
        "    .withColumn(\"lng_round\", round(col(\"lng\").cast(\"double\"), 2)) \\\n",
        "    .withColumn(\"wthr_date\", to_date(\"wthr_date\"))\n",
        "\n",
        "# Step 2.3: Join weather to streaming data\n",
        "joined_stream_df = receipt_2021_stream_df.alias(\"r\").join(\n",
        "    weather_df_cleaned.alias(\"w\"),\n",
        "    (col(\"r.lat_round\") == col(\"w.lat_round\")) &\n",
        "    (col(\"r.lng_round\") == col(\"w.lng_round\")) &\n",
        "    (col(\"r.visit_date\") == col(\"w.wthr_date\")),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Step 2.4: Filter avg temperature > 0\n",
        "joined_stream_df = joined_stream_df.filter(col(\"avg_tmpr_c\") > 0)\n",
        "\n",
        "# Step 2.5: Add original_total_cost\n",
        "joined_stream_df = joined_stream_df.withColumn(\n",
        "    \"original_total_cost\",\n",
        "    col(\"total_cost\").cast(\"double\") + col(\"discount\").cast(\"double\")\n",
        ")\n"
      ],
      "metadata": {
        "id": "I2ghyk5avo1i"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joined_stream_df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye5SrUHBwbzy",
        "outputId": "700f0021-7c65-4428-9633-c951fde3b642"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- franchise_id: string (nullable = true)\n",
            " |-- franchise_name: string (nullable = true)\n",
            " |-- restaurant_franchise_id: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- lng: string (nullable = true)\n",
            " |-- receipt_id: string (nullable = true)\n",
            " |-- total_cost: string (nullable = true)\n",
            " |-- discount: string (nullable = true)\n",
            " |-- date_time: string (nullable = true)\n",
            " |-- lat_round: double (nullable = true)\n",
            " |-- lng_round: double (nullable = true)\n",
            " |-- visit_date: date (nullable = true)\n",
            " |-- lng: string (nullable = true)\n",
            " |-- lat: string (nullable = true)\n",
            " |-- avg_tmpr_c: string (nullable = true)\n",
            " |-- wthr_date: date (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            " |-- lat_round: double (nullable = true)\n",
            " |-- lng_round: double (nullable = true)\n",
            " |-- original_total_cost: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ðŸ”„ Step: Stream Processing for 2021 Receipt Data\n",
        "\n",
        "In this step, we process the 2021 receipt data using Spark Structured Streaming.\n",
        "\n",
        "Since `pivot()` and aggregation functions are **not supported in streaming mode**, we only apply the supported transformations here and write the enriched records to disk. We will apply grouping and summarization in batch mode later.\n",
        "\n",
        "#### Applied Streaming Transformations:\n",
        "- âœ… Added `item_count` using random values (`rand()`).\n",
        "- âœ… Classified `order_type` based on item count:\n",
        "  - \"Erroneous data\": null or <= 0\n",
        "  - \"Tiny order\": 1 or less\n",
        "  - \"Small order\": 1â€“3\n",
        "  - \"Medium order\": 3â€“10\n",
        "  - \"Large order\": > 10\n",
        "- âœ… Added `promo_cold_drinks` based on average temperature:\n",
        "  - `True` if `avg_tmpr_c > 25.0`, else `False`\n",
        "- âœ… Added `batch_timestamp` using `current_timestamp()`\n",
        "- âœ… Wrote the stream output to `/content/output/streamed_2021_data` as CSV\n",
        "\n",
        "The next step will read this output in batch mode and apply the necessary grouping, pivoting, and aggregation logic to complete the task.\n"
      ],
      "metadata": {
        "id": "sK6TemoIxhZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand, col, when\n",
        "\n",
        "# Step 1: Add item_count and classify order_type\n",
        "stream_df = joined_stream_df.withColumn(\n",
        "    \"item_count\", (rand() * 10).cast(\"int\")\n",
        ").withColumn(\n",
        "    \"order_type\",\n",
        "    when(col(\"item_count\").isNull() | (col(\"item_count\") <= 0), \"Erroneous data\")\n",
        "    .when(col(\"item_count\") <= 1, \"Tiny order\")\n",
        "    .when(col(\"item_count\") <= 3, \"Small order\")\n",
        "    .when(col(\"item_count\") <= 10, \"Medium order\")\n",
        "    .otherwise(\"Large order\")\n",
        ")\n",
        "\n",
        "# Step 2: Add promo_cold_drinks\n",
        "stream_df = stream_df.withColumn(\n",
        "    \"promo_cold_drinks\",\n",
        "    when(col(\"avg_tmpr_c\") > 25.0, True).otherwise(False)\n",
        ")\n",
        "\n",
        "# Step 3: Add batch_timestamp\n",
        "from pyspark.sql.functions import current_timestamp\n",
        "\n",
        "stream_df = stream_df.withColumn(\"batch_timestamp\", current_timestamp())\n"
      ],
      "metadata": {
        "id": "ImDmz1M1wjfb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Write streaming result to disk\n",
        "stream_query = stream_df.writeStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"checkpointLocation\", \"/content/output/checkpoint\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start(\"/content/output/streamed_2021_data\")"
      ],
      "metadata": {
        "id": "xnw7-nTvwuK8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the streamed 2021 data from disk\n",
        "streamed_df = spark.read.option(\"header\", True).csv(\"/content/output/streamed_2021_data/*.csv\")\n",
        "\n",
        "# Preview schema\n",
        "streamed_df.printSchema()\n",
        "\n",
        "# Show sample records\n",
        "streamed_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntqgooDxxRQC",
        "outputId": "0c4f73d0-c16a-4742-c56e-cd65c178224e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: string (nullable = true)\n",
            " |-- franchise_id: string (nullable = true)\n",
            " |-- franchise_name: string (nullable = true)\n",
            " |-- restaurant_franchise_id: string (nullable = true)\n",
            " |-- country4: string (nullable = true)\n",
            " |-- city5: string (nullable = true)\n",
            " |-- lat6: string (nullable = true)\n",
            " |-- lng7: string (nullable = true)\n",
            " |-- receipt_id: string (nullable = true)\n",
            " |-- total_cost: string (nullable = true)\n",
            " |-- discount: string (nullable = true)\n",
            " |-- date_time: string (nullable = true)\n",
            " |-- lat_round12: string (nullable = true)\n",
            " |-- lng_round13: string (nullable = true)\n",
            " |-- visit_date: string (nullable = true)\n",
            " |-- lng15: string (nullable = true)\n",
            " |-- lat16: string (nullable = true)\n",
            " |-- avg_tmpr_c: string (nullable = true)\n",
            " |-- wthr_date: string (nullable = true)\n",
            " |-- city19: string (nullable = true)\n",
            " |-- country20: string (nullable = true)\n",
            " |-- lat_round21: string (nullable = true)\n",
            " |-- lng_round22: string (nullable = true)\n",
            " |-- original_total_cost: string (nullable = true)\n",
            " |-- item_count: string (nullable = true)\n",
            " |-- order_type: string (nullable = true)\n",
            " |-- promo_cold_drinks: string (nullable = true)\n",
            " |-- batch_timestamp: string (nullable = true)\n",
            "\n",
            "+------------+------------+------------------+-----------------------+--------+-----------+------+--------+------------------------------------+----------+--------+------------------------+-----------+-----------+----------+--------+------+----------+----------+-----------+---------+-----------+-----------+-------------------+----------+------------+-----------------+------------------------+\n",
            "|id          |franchise_id|franchise_name    |restaurant_franchise_id|country4|city5      |lat6  |lng7    |receipt_id                          |total_cost|discount|date_time               |lat_round12|lng_round13|visit_date|lng15   |lat16 |avg_tmpr_c|wthr_date |city19     |country20|lat_round21|lng_round22|original_total_cost|item_count|order_type  |promo_cold_drinks|batch_timestamp         |\n",
            "+------------+------------+------------------+-----------------------+--------+-----------+------+--------+------------------------------------+----------+--------+------------------------+-----------+-----------+----------+--------+------+----------+----------+-----------+---------+-----------+-----------+-------------------+----------+------------+-----------------+------------------------+\n",
            "|77309411383 |56          |The Waffle House  |63415                  |GB      |London     |51.461|-0.276  |885ffe85-3320-49a5-bc91-8f8ed227af5a|15.65     |0.0     |2021-10-11T03:45:43.000Z|51.46      |-0.28      |2021-10-11|-0.276  |51.461|10.81     |2021-10-11|London     |GB       |51.46      |-0.28      |15.65              |1         |Tiny order  |false            |2025-03-23T06:51:00.490Z|\n",
            "|103079215117|14          |The Gourmet Garden|1670                   |US      |Victorville|34.507|-117.326|7e4432fc-0649-4eab-883c-e3e975501413|27.93     |0.0     |2022-09-01T08:29:03.000Z|34.51      |-117.33    |2022-09-01|-117.326|34.507|30.01     |2022-09-01|Victorville|US       |34.51      |-117.33    |27.93              |7         |Medium order|true             |2025-03-23T06:51:00.490Z|\n",
            "|120259084341|54          |The Spice House   |40542                  |GB      |London     |51.502|-0.119  |d7da2e39-1c0c-4f11-8826-d174abc97e40|21.84     |0.0     |2022-08-17T15:04:06.000Z|51.5       |-0.12      |2022-08-17|-0.117  |51.501|15.91     |2022-08-17|London     |GB       |51.5       |-0.12      |21.84              |7         |Medium order|false            |2025-03-23T06:51:00.490Z|\n",
            "|120259084341|54          |The Spice House   |40542                  |GB      |London     |51.502|-0.119  |d7da2e39-1c0c-4f11-8826-d174abc97e40|21.84     |0.0     |2022-08-17T15:04:06.000Z|51.5       |-0.12      |2022-08-17|-0.116  |51.501|16.44     |2022-08-17|London     |GB       |51.5       |-0.12      |21.84              |3         |Small order |false            |2025-03-23T06:51:00.490Z|\n",
            "|120259084341|54          |The Spice House   |40542                  |GB      |London     |51.502|-0.119  |d7da2e39-1c0c-4f11-8826-d174abc97e40|21.84     |0.0     |2022-08-17T15:04:06.000Z|51.5       |-0.12      |2022-08-17|-0.119  |51.495|16.03     |2022-08-17|London     |GB       |51.5       |-0.12      |21.84              |4         |Medium order|false            |2025-03-23T06:51:00.490Z|\n",
            "+------------+------------+------------------+-----------------------+--------+-----------+------+--------+------------------------------------+----------+--------+------------------------+-----------+-----------+----------+--------+------+----------+----------+-----------+---------+-----------+-----------+-------------------+----------+------------+-----------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "order_counts_streamed.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcxRWVi4zysa",
        "outputId": "f7a34c2d-6ff7-461a-f110-63cf1838b5dd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- restaurant_franchise_id: string (nullable = true)\n",
            " |-- Erroneous data: long (nullable = true)\n",
            " |-- Medium order: long (nullable = true)\n",
            " |-- Small order: long (nullable = true)\n",
            " |-- Tiny order: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Streaming Aggregation\n",
        "\n",
        "In this step, we:\n",
        "- Group the streamed data by `restaurant_franchise_id`\n",
        "- Count the number of orders in each category (erroneous, tiny, small, medium, large)\n",
        "- Identify the most popular order type\n",
        "- Retain `promo_cold_drinks` and `batch_timestamp` for each restaurant\n",
        "- Save the enriched streaming state to `/content/output/streamed_2021_state`\n"
      ],
      "metadata": {
        "id": "FBYkYjgp0aBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, when, count, current_timestamp, expr\n",
        "\n",
        "# Step 1: Reclassify orders into states (just like 2022 logic)\n",
        "streamed_agg_df = streamed_df.withColumn(\n",
        "    \"order_state\",\n",
        "    when(col(\"item_count\").isNull() | (col(\"item_count\") <= 0), \"Erroneous data\")\n",
        "    .when(col(\"item_count\") <= 1, \"Tiny order\")\n",
        "    .when(col(\"item_count\") <= 3, \"Small order\")\n",
        "    .when(col(\"item_count\") <= 10, \"Medium order\")\n",
        "    .otherwise(\"Large order\")\n",
        ")\n",
        "\n",
        "# Step 2: Group and pivot to count each order type\n",
        "order_counts_streamed = streamed_agg_df.groupBy(\"restaurant_franchise_id\") \\\n",
        "    .pivot(\"order_state\").count()\n",
        "\n",
        "# Step 3: Add most popular order type\n",
        "order_counts_streamed = order_counts_streamed \\\n",
        "    .withColumnRenamed(\"Erroneous data\", \"erroneous_data\") \\\n",
        "    .withColumnRenamed(\"Medium order\", \"medium_order\") \\\n",
        "    .withColumnRenamed(\"Small order\", \"small_order\") \\\n",
        "    .withColumnRenamed(\"Tiny order\", \"tiny_order\"\n",
        ")\n",
        "\n",
        "# Step 4: Join with promo_cold_drinks and batch_timestamp\n",
        "extras = streamed_df.select(\"restaurant_franchise_id\", \"promo_cold_drinks\", \"batch_timestamp\").dropDuplicates()\n",
        "\n",
        "final_streamed_state_df = order_counts_streamed.join(\n",
        "    extras,\n",
        "    on=\"restaurant_franchise_id\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Step 5: Save result\n",
        "final_streamed_state_df.coalesce(1) \\\n",
        "    .write.mode(\"overwrite\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .csv(\"/content/output/streamed_2021_state\")"
      ],
      "metadata": {
        "id": "WWP6sFNSzAQP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_streamed_state_df.show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcbKbfaXzQuq",
        "outputId": "bff76f50-c585-464a-84a4-8744ac46e0f5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------+--------------+------------+-----------+----------+-----------------+------------------------+\n",
            "|restaurant_franchise_id|erroneous_data|medium_order|small_order|tiny_order|promo_cold_drinks|batch_timestamp         |\n",
            "+-----------------------+--------------+------------+-----------+----------+-----------------+------------------------+\n",
            "|22875                  |252           |1503        |485        |256       |false            |2025-03-23T06:51:00.490Z|\n",
            "|22875                  |252           |1503        |485        |256       |true             |2025-03-23T06:51:00.490Z|\n",
            "|76199                  |245           |1469        |467        |257       |true             |2025-03-23T06:51:00.490Z|\n",
            "|76199                  |245           |1469        |467        |257       |false            |2025-03-23T06:51:00.490Z|\n",
            "|23242                  |569           |3402        |1071       |555       |false            |2025-03-23T06:51:00.490Z|\n",
            "+-----------------------+--------------+------------+-----------+----------+-----------------+------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/final_results.zip /content/output/final_state /content/output/streamed_2021_state\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI5tVybm0dht",
        "outputId": "0faf2c1b-7e0a-413a-ff7f-941a36e14ed5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/output/final_state/ (stored 0%)\n",
            "  adding: content/output/final_state/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/final_state/_SUCCESS (stored 0%)\n",
            "  adding: content/output/final_state/part-00000-e3f6b190-6646-4291-8d32-266e46724fe7-c000.csv (deflated 58%)\n",
            "  adding: content/output/final_state/.part-00000-e3f6b190-6646-4291-8d32-266e46724fe7-c000.csv.crc (stored 0%)\n",
            "  adding: content/output/streamed_2021_state/ (stored 0%)\n",
            "  adding: content/output/streamed_2021_state/._SUCCESS.crc (stored 0%)\n",
            "  adding: content/output/streamed_2021_state/part-00000-2838af3c-6df5-4021-87d8-d4fddbd11665-c000.csv (deflated 80%)\n",
            "  adding: content/output/streamed_2021_state/_SUCCESS (stored 0%)\n",
            "  adding: content/output/streamed_2021_state/.part-00000-2838af3c-6df5-4021-87d8-d4fddbd11665-c000.csv.crc (stored 0%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/final_results.zip')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xGtJsTfE13k5",
        "outputId": "fb1bbb88-d1d6-4926-8bcf-cf1e206cc654"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9f3e0416-ca7c-4f43-8667-0beb74f575b5\", \"final_results.zip\", 3806)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPhz0hV52Iug"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}